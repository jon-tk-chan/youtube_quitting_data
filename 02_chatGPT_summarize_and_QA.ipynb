{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "050a7479",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import json \n",
    "import pandas as pd\n",
    "import time\n",
    "from credentials import OPENAI_KEY\n",
    "import os\n",
    "\n",
    "\n",
    "openai.api_key = OPENAI_KEY\n",
    "model_engine=\"text-davinci-003\"\n",
    "\n",
    "max_summaries = 10\n",
    "\n",
    "in_filepath = \"test_data/01_youtubeStatsTranscripts.json\"\n",
    "\n",
    "summ_filepath = \"test_data/summaries\"\n",
    "if not os.path.exists(summ_filepath):\n",
    "    os.makedirs(summ_filepath)\n",
    "    \n",
    "qa_filepath = \"test_data/QA_out\"\n",
    "if not os.path.exists(qa_filepath):\n",
    "    os.makedirs(qa_filepath)\n",
    "    \n",
    "out_filepath = \"test_data/02_QAOutputChatGPT.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8feee166",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90d618ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(in_filepath, 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d60847ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()\n",
    "len(data['videoId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65daab50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "\n",
    "####SUMMARIZE LONGER TRANSCRIPTS - store in summaries_dict\n",
    "#### save checkpoints to summaries folder\n",
    "all_summaries = []\n",
    "summaries_dict = {\n",
    "    \n",
    "}\n",
    "max_len = 1800\n",
    "threshold_perc = 0.75\n",
    "threshold = max_len*threshold_perc\n",
    "chunksize = 1000\n",
    "ind = 0 #used to iterate through transcript chunks to summarize\n",
    "\n",
    "# summary_prefix = f\"\"\"This is a transcript from a YouTube video.in {threshold} words or less, \n",
    "#                     summarize the following text: \"\"\"\n",
    "# classify_prefix = \"This is a transcript from a YouTube video.\"\n",
    "for i,text in enumerate(data['transcript_strings']):\n",
    "    video_id = data['videoId'][i]\n",
    "    \n",
    "#     if text and i <=250:\n",
    "    if text and i <= max_summaries:\n",
    "        tokens = text.split()\n",
    "#         print(f\"FINAL LENGTH: {len(tokens)}\")\n",
    "        if len(tokens) >= threshold_perc:\n",
    "            summary_chunks = []\n",
    "            for i in range(0, len(tokens),chunksize):\n",
    "                ###SET indices to get tokens for each chunk\n",
    "                if i == 0:\n",
    "#                     print(\"---START\")\n",
    "                    start_ind = i\n",
    "                    end_ind = chunksize\n",
    "                elif len(tokens) % i < chunksize and len(tokens)//i <= 1.0:\n",
    "#                     print(\"---END\")\n",
    "                    start_ind = i +1\n",
    "                    end_ind = len(tokens)\n",
    "                else:\n",
    "#                     print(\"---MID\")\n",
    "                    start_ind = i +1\n",
    "                    end_ind = i + chunksize\n",
    "#                 print(f\"START AND END: {start_ind} to {end_ind}\")\n",
    "#                 print(\"****\")\n",
    "                to_summarize = \" \".join(tokens[start_ind:end_ind])\n",
    "#                 print(to_summarize)\n",
    "#                 print(\"****\")\n",
    "\n",
    "                summ_query = f\"The following is a portion of a YouTube video transcript. Summarize what the speaker is saying and include as many details as you can about the speaker's current and former job positions. Also include details related to their reasons for quitting: {to_summarize}\"\n",
    "#                 print(len(summ_query.split()))\n",
    "                summary_completion = openai.Completion.create(\n",
    "                    engine=model_engine,\n",
    "                    prompt=summ_query,\n",
    "                    max_tokens=500, #prompt restricts length of response, but max_tokens determines cutoff tokens \n",
    "                    n=1,\n",
    "                    stop=None,\n",
    "                    temperature=0.5,\n",
    "                )\n",
    "#                 print(summary_completion)\n",
    "                summary_chunks.append(summary_completion.choices[0].text)\n",
    "            all_summaries.append(\"\\n\".join(summary_chunks))\n",
    "            summaries_dict[video_id] = \"\\n\".join(summary_chunks)\n",
    "#             print(len(\"\\n\".join(summary_chunks).split()))\n",
    "        else:\n",
    "            print(\"CLASSIFY\")\n",
    "            continue\n",
    "        print(\"---\")\n",
    "    else:\n",
    "#         print(\"NO TRANSCRIPT\")\n",
    "        summaries_dict[video_id] = \"NO TRANSCRIPT AVAILABLE\"\n",
    "    \n",
    "    ###ADD CHECKPOINT IN 'summaries' file - put last video id into file name\n",
    "    if len(str(ind)) ==1:\n",
    "        checkpoint_ind = \"00\" + str(ind)\n",
    "    elif len(str(ind)) == 2:\n",
    "        checkpoint_ind = \"0\" + str(ind)\n",
    "    else:\n",
    "        checkpoint_ind = str(ind)\n",
    "    checkpoint_file = f\"{summ_filepath}/checkpoint_{checkpoint_ind}_{video_id}.json\"\n",
    "    with open(checkpoint_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(summaries_dict, f, ensure_ascii=False, indent=4)\n",
    "    \n",
    "    if ind == max_summaries:\n",
    "        break\n",
    "    ind += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e6654a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3e5535",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02eb49a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b0b988fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = summaries.values()\n",
    "# qa_query = f\"This is a transcript from a YouTube video. in three separate bullet points, answer 3 questions for the following transcript: \\n What is the current job/occupation of the speaker? \\n 2. What was the former job/occupation of the speaker? \\n 3. What is the main reason that the speaker had for quitting their job/occupation? \\n {text}\"\n",
    "    \n",
    "# qa_completion = openai.Completion.create(\n",
    "#     engine=model_engine,\n",
    "#     prompt=qa_query,\n",
    "#     max_tokens=500, #prompt restricts length of response, but max_tokens determines cutoff tokens \n",
    "#     n=1,\n",
    "#     stop=None,\n",
    "#     temperature=0.5,\n",
    "# )\n",
    "\n",
    "# qa_result = qa_completion.choices[0].text\n",
    "# qa_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04f5d8e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "#answer questions\n",
    "#get last item from chkpt_filpath - last checkpoint w most summaries \n",
    "final_summaries = sorted(os.listdir(summ_filepath))[-1]\n",
    "summaries_filepath=f\"{summ_filepath}/{final_summaries}\"\n",
    "with open(summaries_filepath, 'r') as f:\n",
    "    summaries = json.load(f)\n",
    "\n",
    "qa_dict = {}\n",
    "qa_ind = 0 \n",
    "for video_id, text in summaries.items():\n",
    "    qa_query = f\"This is a transcript from a YouTube video. in three separate bullet points, answer 3 questions for the following transcript: \\n What is the current job/occupation of the speaker? \\n 2. What was the former job/occupation of the speaker? \\n 3. What is the main reason that the speaker had for quitting their job/occupation? \\n {text}\"\n",
    "    if len(qa_query.split()) <= 3000:\n",
    "        qa_completion = openai.Completion.create(\n",
    "            engine=model_engine,\n",
    "            prompt=qa_query,\n",
    "            max_tokens=500, #prompt restricts length of response, but max_tokens determines cutoff tokens \n",
    "            n=1,\n",
    "            stop=None,\n",
    "            temperature=0.5,\n",
    "        )\n",
    "\n",
    "        qa_result = qa_completion.choices[0].text\n",
    "#         print(qa_result)\n",
    "        if len(qa_result) > 0:\n",
    "            qa_dict[video_id] = qa_result\n",
    "        else:\n",
    "            qa_dict[video_id] = \"No Answer Available\"\n",
    "\n",
    "        if qa_ind % 10 == 0 or qa_ind == 500:\n",
    "            if len(str(qa_ind)) ==1:\n",
    "                checkpoint_ind = \"00\" + str(qa_ind)\n",
    "            elif len(str(qa_ind)) == 2:\n",
    "                checkpoint_ind = \"0\" + str(qa_ind)\n",
    "            else:\n",
    "                checkpoint_ind = str(qa_ind)\n",
    "            checkpoint_file = f\"{qa_filepath}/checkpoint_{checkpoint_ind}_{video_id}.json\"\n",
    "\n",
    "            with open(checkpoint_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(qa_dict, f, ensure_ascii=False, indent=4)\n",
    "        print(\"---\")\n",
    "    else:\n",
    "        print(\"****\")\n",
    "        print(f'Transcript is too long: {video_id} {len(qa_query.split())}')\n",
    "        print(qa_query)\n",
    "        continue\n",
    "    qa_ind += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84cda158",
   "metadata": {},
   "outputs": [],
   "source": [
    "#refactor to save at end as well\n",
    "with open(out_filepath, 'w', encoding='utf-8') as f:\n",
    "    json.dump(qa_dict, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ac610a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
