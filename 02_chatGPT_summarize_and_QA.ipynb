{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "050a7479",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import json \n",
    "import pandas as pd\n",
    "import time\n",
    "import credentials \n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "import os\n",
    "\n",
    "imported_key = credentials.OPENAI_KEY\n",
    "OpenAI.api_key = imported_key\n",
    "# model_engine=\"text-davinci-003\"\n",
    "\n",
    "# max_summaries = 10\n",
    "data_folder = \"data\"\n",
    "#Replace with \"data folder\" when doing final run - will incur OpenAI charges for token usage\n",
    "# max_summaries = 250\n",
    "# data_folder = \"data\"\n",
    "# in_filepath = f\"{data_folder}/01_youtubeStatsTranscripts.json\"\n",
    "\n",
    "# summ_filepath = f\"{data_folder}/summaries\"\n",
    "# if not os.path.exists(summ_filepath):\n",
    "#     os.makedirs(summ_filepath)\n",
    "    \n",
    "# qa_filepath = f\"{data_folder}/QA_out\"\n",
    "# if not os.path.exists(qa_filepath):\n",
    "#     os.makedirs(qa_filepath)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "client = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=imported_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8feee166",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>video_id</th>\n",
       "      <th>published_at</th>\n",
       "      <th>channel_id</th>\n",
       "      <th>channel_name</th>\n",
       "      <th>Category</th>\n",
       "      <th>favourites</th>\n",
       "      <th>title</th>\n",
       "      <th>views</th>\n",
       "      <th>likes</th>\n",
       "      <th>dislikes</th>\n",
       "      <th>comment_count</th>\n",
       "      <th>description</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>bVJfQAe-UP4</td>\n",
       "      <td>2022-12-20T12:00:33Z</td>\n",
       "      <td>UC7cs8q-gJRlGwj4A8OmCmXg</td>\n",
       "      <td>Alex The Analyst</td>\n",
       "      <td>Education</td>\n",
       "      <td>0</td>\n",
       "      <td>Why I Quit my 125k Analytics Job</td>\n",
       "      <td>158087</td>\n",
       "      <td>8715</td>\n",
       "      <td>Not available</td>\n",
       "      <td>858</td>\n",
       "      <td>This is not where I saw my career going, but h...</td>\n",
       "      <td>['Data Analyst', 'Data Analyst job', 'Data Ana...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-ayEjbg0ZEc</td>\n",
       "      <td>2021-07-22T14:00:01Z</td>\n",
       "      <td>UChpxXPOP1xIq65mpNv-DclQ</td>\n",
       "      <td>NFTs Simplified</td>\n",
       "      <td>People &amp; Blogs</td>\n",
       "      <td>0</td>\n",
       "      <td>I QUIT MY JOB as a Financial Analyst to START ...</td>\n",
       "      <td>3968</td>\n",
       "      <td>211</td>\n",
       "      <td>Not available</td>\n",
       "      <td>61</td>\n",
       "      <td>I quit my job as a financial analyst to start ...</td>\n",
       "      <td>['should you quit your job', 'should i quit my...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>C9h0uhjdsOI</td>\n",
       "      <td>2022-03-07T17:43:04Z</td>\n",
       "      <td>UC0GmdVKZhMM3Rmielp4oVAA</td>\n",
       "      <td>Stefanovic</td>\n",
       "      <td>People &amp; Blogs</td>\n",
       "      <td>0</td>\n",
       "      <td>The REAL Reason I Quit My 6-Figure Data Analys...</td>\n",
       "      <td>70501</td>\n",
       "      <td>3421</td>\n",
       "      <td>Not available</td>\n",
       "      <td>204</td>\n",
       "      <td>Save 25% off Datacamp here:\\nhttps://www.datac...</td>\n",
       "      <td>No Tags</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>sd5F1uR3tvA</td>\n",
       "      <td>2024-03-23T11:22:46Z</td>\n",
       "      <td>UCNXDU-8M1KFAWpFxmev8C-g</td>\n",
       "      <td>Jess Ramos</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>0</td>\n",
       "      <td>I quit my 6-figure Data Analyst job in 4 month...</td>\n",
       "      <td>91</td>\n",
       "      <td>3</td>\n",
       "      <td>Not available</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No Tags</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>M8md7_gyBy4</td>\n",
       "      <td>2022-01-17T10:58:08Z</td>\n",
       "      <td>UC0GmdVKZhMM3Rmielp4oVAA</td>\n",
       "      <td>Stefanovic</td>\n",
       "      <td>People &amp; Blogs</td>\n",
       "      <td>0</td>\n",
       "      <td>I QUIT my $170,000 Tech Job After Learning 3 L...</td>\n",
       "      <td>1011039</td>\n",
       "      <td>41165</td>\n",
       "      <td>Not available</td>\n",
       "      <td>1875</td>\n",
       "      <td>Save 25% off Datacamp here:\\nhttps://datacamp....</td>\n",
       "      <td>['i quit my job', 'passive income', 'passive i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     video_id          published_at                channel_id  \\\n",
       "0           0  bVJfQAe-UP4  2022-12-20T12:00:33Z  UC7cs8q-gJRlGwj4A8OmCmXg   \n",
       "1           1  -ayEjbg0ZEc  2021-07-22T14:00:01Z  UChpxXPOP1xIq65mpNv-DclQ   \n",
       "2           2  C9h0uhjdsOI  2022-03-07T17:43:04Z  UC0GmdVKZhMM3Rmielp4oVAA   \n",
       "3           3  sd5F1uR3tvA  2024-03-23T11:22:46Z  UCNXDU-8M1KFAWpFxmev8C-g   \n",
       "4           4  M8md7_gyBy4  2022-01-17T10:58:08Z  UC0GmdVKZhMM3Rmielp4oVAA   \n",
       "\n",
       "       channel_name        Category  favourites  \\\n",
       "0  Alex The Analyst       Education           0   \n",
       "1   NFTs Simplified  People & Blogs           0   \n",
       "2        Stefanovic  People & Blogs           0   \n",
       "3        Jess Ramos   Entertainment           0   \n",
       "4        Stefanovic  People & Blogs           0   \n",
       "\n",
       "                                               title    views  likes  \\\n",
       "0                   Why I Quit my 125k Analytics Job   158087   8715   \n",
       "1  I QUIT MY JOB as a Financial Analyst to START ...     3968    211   \n",
       "2  The REAL Reason I Quit My 6-Figure Data Analys...    70501   3421   \n",
       "3  I quit my 6-figure Data Analyst job in 4 month...       91      3   \n",
       "4  I QUIT my $170,000 Tech Job After Learning 3 L...  1011039  41165   \n",
       "\n",
       "        dislikes  comment_count  \\\n",
       "0  Not available            858   \n",
       "1  Not available             61   \n",
       "2  Not available            204   \n",
       "3  Not available              0   \n",
       "4  Not available           1875   \n",
       "\n",
       "                                         description  \\\n",
       "0  This is not where I saw my career going, but h...   \n",
       "1  I quit my job as a financial analyst to start ...   \n",
       "2  Save 25% off Datacamp here:\\nhttps://www.datac...   \n",
       "3                                                NaN   \n",
       "4  Save 25% off Datacamp here:\\nhttps://datacamp....   \n",
       "\n",
       "                                                tags  \n",
       "0  ['Data Analyst', 'Data Analyst job', 'Data Ana...  \n",
       "1  ['should you quit your job', 'should i quit my...  \n",
       "2                                            No Tags  \n",
       "3                                            No Tags  \n",
       "4  ['i quit my job', 'passive income', 'passive i...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_filename = f\"{data_folder}/01_youtube_stats_df.csv\"\n",
    "in_df = pd.read_csv(input_filename)\n",
    "in_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b0011627-8b31-4361-972e-69de0ccd256c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MISSING 5w7_SHg5S3c - SKIPPED\n",
      "MISSING VPQaE6TJGaU - SKIPPED\n",
      "MISSING oxXQQEethiY - SKIPPED\n",
      "MISSING VjDJC2rkrzE - SKIPPED\n",
      "MISSING X5gJgADzzPg - SKIPPED\n",
      "MISSING t5tA8DBavr0 - SKIPPED\n",
      "MISSING DOrkPlUnBsA - SKIPPED\n",
      "MISSING MdKKjzcAHhA - SKIPPED\n",
      "MISSING 1t05sfYZ3Rg - SKIPPED\n",
      "MISSING __sXIwUpZ44 - SKIPPED\n",
      "MISSING 1iqA7caXOX8 - SKIPPED\n",
      "MISSING wAJs1WpOP9o - SKIPPED\n",
      "MISSING p3ss8nLXpGE - SKIPPED\n",
      "MISSING p3ss8nLXpGE - SKIPPED\n",
      "MISSING QT5nReXFK5U - SKIPPED\n",
      "MISSING f4o7ervFv8M - SKIPPED\n",
      "MISSING J69hQTtwaUg - SKIPPED\n",
      "MISSING FjjUHY0wDQk - SKIPPED\n",
      "MISSING w5lCSgk0V4Y - SKIPPED\n",
      "MISSING BQ6upZcbsdI - SKIPPED\n",
      "MISSING P6u0CHH6Xm0 - SKIPPED\n",
      "MISSING 9NMmGboQbTs - SKIPPED\n",
      "MISSING FKltifMzM2M - SKIPPED\n",
      "MISSING BQ6upZcbsdI - SKIPPED\n",
      "MISSING P6u0CHH6Xm0 - SKIPPED\n",
      "MISSING HIL4E5UByp8 - SKIPPED\n",
      "MISSING LK9Ww_gfLfk - SKIPPED\n",
      "MISSING 8GzCa2XijAs - SKIPPED\n",
      "MISSING xHrdq62R-FE - SKIPPED\n",
      "MISSING k8ahtT4tft8 - SKIPPED\n",
      "MISSING fiGrD9_fE6o - SKIPPED\n",
      "MISSING mKkjJlqCepw - SKIPPED\n",
      "MISSING 27fd9D57xHw - SKIPPED\n",
      "MISSING ClWSYNeMzBc - SKIPPED\n",
      "MISSING pnxxGXDmctw - SKIPPED\n",
      "MISSING qEIDYAMmulU - SKIPPED\n",
      "MISSING aaDCPHGyRFs - SKIPPED\n",
      "MISSING gaTGppl93Xo - SKIPPED\n",
      "MISSING jyV3H51tS4E - SKIPPED\n",
      "MISSING 5xLWkLpBlSY - SKIPPED\n",
      "MISSING 7sAiGVisfe0 - SKIPPED\n",
      "MISSING v32M3ArPkHc - SKIPPED\n",
      "MISSING l99q5lthTM0 - SKIPPED\n",
      "MISSING INU_hyBJ-FY - SKIPPED\n",
      "MISSING qqxNK7xkRDo - SKIPPED\n",
      "MISSING 76TTNixvGVI - SKIPPED\n",
      "MISSING Tm3X-Gdv9Ig - SKIPPED\n",
      "MISSING VtnQv-CN_NE - SKIPPED\n",
      "MISSING xP5ZHDw-XOQ - SKIPPED\n",
      "MISSING L1qdMwduw4Q - SKIPPED\n",
      "MISSING RmG3qdWuUZY - SKIPPED\n",
      "MISSING wIXp1OtP2Xg - SKIPPED\n",
      "MISSING W84dgS0Nm0o - SKIPPED\n",
      "MISSING L3zAtUy9MXU - SKIPPED\n",
      "MISSING ccVk_yqBKac - SKIPPED\n",
      "MISSING n5tuPsLNs7E - SKIPPED\n",
      "MISSING O7VQjGxl-CU - SKIPPED\n",
      "MISSING 1IaF1oGDkVI - SKIPPED\n",
      "MISSING CMaNxoae4_M - SKIPPED\n",
      "MISSING h6O3EaKc-NA - SKIPPED\n",
      "MISSING L5o3gPFxe_M - SKIPPED\n",
      "MISSING uGeNSOkQNOs - SKIPPED\n",
      "MISSING AmvaJDftusg - SKIPPED\n",
      "MISSING 6RMz9HoGnY0 - SKIPPED\n",
      "MISSING yeFcbh3T0rE - SKIPPED\n",
      "MISSING 5qAh9se3epI - SKIPPED\n",
      "MISSING h6O3EaKc-NA - SKIPPED\n",
      "MISSING L5o3gPFxe_M - SKIPPED\n",
      "MISSING 7qFZvUCmyEo - SKIPPED\n",
      "MISSING LVFQu_piWsE - SKIPPED\n",
      "MISSING 1IaF1oGDkVI - SKIPPED\n",
      "MISSING AmvaJDftusg - SKIPPED\n",
      "MISSING 6RMz9HoGnY0 - SKIPPED\n",
      "MISSING 9AiybTTfV2c - SKIPPED\n",
      "MISSING yeFcbh3T0rE - SKIPPED\n",
      "MISSING FiSb4V25iTk - SKIPPED\n",
      "MISSING GAfbM5gONHc - SKIPPED\n",
      "MISSING 0m7tN6riipc - SKIPPED\n"
     ]
    }
   ],
   "source": [
    "def scrape_transcripts(input_df): \n",
    "    \"\"\"Returns a dict of dicts containing the video ids, transcripts as a single string, \n",
    "    and transcripts as individual sents (list of strings)\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    ##Get transcripts using get_transcript() of YoutubeTranscriptApi for each transcript\n",
    "    ### Get as dictionary of dictionary: youtube_id : {}\n",
    "    \n",
    "    transcript_dict = {}\n",
    "    yt_api = YouTubeTranscriptApi()\n",
    "    \n",
    "    for i, video_id in enumerate(input_df['video_id']):\n",
    "        transcript_sents = None #create new list for individual sentences\n",
    "        transcript_joined = None\n",
    "        try:\n",
    "            \n",
    "            curr_result = yt_api.get_transcript(video_id,languages=['en'])\n",
    "            # print(curr_result)\n",
    "            transcript_sents = [str(x['text']).replace(\"\\xa0\", \"\") for x in curr_result]\n",
    "            transcript_joined = \" \".join(transcript_sents)\n",
    "            # print(f\"WORKING - {video_id}\")\n",
    "            # print(f\"FIRST SENTS FOR {video_id} TRANSCRIPT: {transcript_sents[:5]}\")\n",
    "            \n",
    "            \n",
    "        except:\n",
    "            print(f\"MISSING {video_id} - SKIPPED\")\n",
    "        transcript_dict[video_id] = {\n",
    "                                        \"joined\": transcript_joined,\n",
    "                                        \"sents\": transcript_sents\n",
    "            \n",
    "                                    }\n",
    "    \n",
    "        # if i == 5:\n",
    "        #     break\n",
    "        if i % 50 == 0:\n",
    "            print(f\"PROCESSED {i} TRANSCRIPTS\")\n",
    "            print(f\"-----\")\n",
    "    \n",
    "    # print(len(transcript_dict))\n",
    "    return transcript_dict\n",
    "transcript_dict = scrape_transcripts(in_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "15bda9b6-d14f-4e2b-9770-1734a664263a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "youtube_id: bVJfQAe-UP4, number of sents: 235\n",
      "first sents[\"hello everybody yes I'm making one of\", 'those videos and it honestly just feels', 'a bit surreal it feels like just', 'yesterday when I got my first job as a', 'data analyst but that was over five']\n",
      "number of tokens: 1764\n",
      "----\n",
      "youtube_id: -ayEjbg0ZEc, number of sents: 279\n",
      "first sents[\"if you're considering quitting your\", 'cushy full-time job in order to pursue', 'your own business', 'this video is for you', '[Music]']\n",
      "number of tokens: 1614\n",
      "----\n",
      "youtube_id: C9h0uhjdsOI, number of sents: 97\n",
      "first sents['i lied in one of my previous videos i explained\\nwhy i quit my six-figure job i explained how', \"over the years i've learned three very important\\nlessons that made me decide to leave everything\", \"behind and start building a life i actually want\\nbut i wasn't completely honest now let me explain\", \"you see as i said in that video there's three\\nlessons i learned that made me decide to quit\", 'my job first one being your permanent job is not\\nso permanent your permanent job is not as safe']\n",
      "number of tokens: 1725\n",
      "----\n",
      "youtube_id: sd5F1uR3tvA, number of sents: 3\n",
      "first sents['walk in the', \"room and I'm glowing like a million\", 'fireflies']\n",
      "number of tokens: 11\n",
      "----\n",
      "youtube_id: M8md7_gyBy4, number of sents: 159\n",
      "first sents['see i was making about 150 000 euros a year just\\nafter a couple of years of working experience as', 'a data analyst eventually working as a\\nfreelancer for a big bank in Amsterdam', \"i honestly thought i had it\\nall now earning more than i've\", 'ever could dream of i bought my own house i travel\\na lot i even have some extra money to invest', \"and i've quit i've left everything behind\\nand i've quit my job before it's too late\"]\n",
      "number of tokens: 2962\n",
      "----\n",
      "youtube_id: mXEhbsDB1ag, number of sents: 237\n",
      "first sents[\"I'm about to resign I'm also letting go\", 'of a sixf figure', \"bonus a sixf figure sign on that's Beau\", \"be paid to me in March I've just sent my\", \"manager I'm shaking I've just sent my\"]\n",
      "number of tokens: 1730\n",
      "----\n",
      "youtube_id: UQ0OnHgawO4, number of sents: 33\n",
      "first sents['so I recently quit my job as a cyber', 'security analyst and because I also made', 'content about my job as a security', 'analyst on both YouTube and Instagram I', 'wanted to share this update with you']\n",
      "number of tokens: 239\n",
      "----\n",
      "youtube_id: 5qlKRHsClQw, number of sents: 3\n",
      "first sents[\"I'm\", 'free to do what I want to', \"do it's my life\"]\n",
      "number of tokens: 12\n",
      "----\n",
      "youtube_id: OMI4Wu9wnY0, number of sents: 278\n",
      "first sents[\"good morning I didn't sleep that well\", 'last night because I am quitting my job', 'today', '[Music]', 'I am so nervous right']\n",
      "number of tokens: 2032\n",
      "----\n",
      "youtube_id: 85TImi32qhs, number of sents: 32\n",
      "first sents[\"I just want to let you all know that I'm\", \"quitting for the past couple years I've\", 'been working two jobs one job as a', 'financial analyst and one job as an', 'internet Grilled Cheese Cook']\n",
      "number of tokens: 242\n",
      "----\n",
      "youtube_id: N5LAj-r0No4, number of sents: 453\n",
      "first sents['hi everyone welcome back to my Channel', \"today I want to share something that's\", 'really personal the reasons why I', 'decided to quit my six fig da scientist', 'job in Tech without another one lined up']\n",
      "number of tokens: 3438\n",
      "----\n",
      "youtube_id: hHzINiemlpM, number of sents: 231\n",
      "first sents[\"hello everybody yes I'm making one of\", 'those videos and it honestly just feels', 'a bit surreal it feels like just', 'yesterday when I got my first job as a', 'data analyst but that was over 5 years']\n",
      "number of tokens: 1773\n",
      "----\n",
      "youtube_id: MRt4l1t8z4M, number of sents: 255\n",
      "first sents['I was making six figures fresh out of', 'college working as an analyst at a', 'prestigious Wall Street Investment Bank', 'it was in many ways the dream set up and', 'everything I had worked for']\n",
      "number of tokens: 1806\n",
      "----\n",
      "youtube_id: _m3P2zVz_94, number of sents: 25\n",
      "first sents['please never forget that you are worth', 'so much more than your paycheck many of', 'you know I made a pretty big career', 'change about a month ago and up until', \"today I've never told anyone exactly\"]\n",
      "number of tokens: 186\n",
      "----\n",
      "TRANSCRIPT MISSING FOR 5w7_SHg5S3c\n",
      "youtube_id: xoUwGx_eLiw, number of sents: 237\n",
      "first sents[\"in this video I'm going to talk about\", 'business analysts career paths and I', 'also want to share some advice with you', 'on top of this I want to share the path', 'which I have taken until this date and']\n",
      "number of tokens: 1658\n",
      "----\n",
      "youtube_id: YsRDM6eoRq8, number of sents: 32\n",
      "first sents['three reasons you should not become a', \"data analyst you don't like building\", 'dashboards believe me if you become a', 'data analyst a large chunk of your time', 'will be dedicated to building reports']\n",
      "number of tokens: 211\n",
      "----\n",
      "youtube_id: pfhUh0Lm4hY, number of sents: 513\n",
      "first sents[\"so it's short I quit again yeah that's\", 'right I quit my second cyber security', 'job', 'and', 'this is pretty much a year yeah yeah so']\n",
      "number of tokens: 3790\n",
      "----\n",
      "youtube_id: 2koX1M96c4A, number of sents: 194\n",
      "first sents['in 2015 I was on track to make six', 'figures working as a consultant it was', 'my first job out of college and my first', 'Real World experience that I was really', \"excited you're told to work really hard\"]\n",
      "number of tokens: 1423\n",
      "----\n",
      "youtube_id: 70miRJ0gMBU, number of sents: 456\n",
      "first sents['foreign', 'I had to sit down with myself and be', 'like girl', 'you can do better', 'okay I quit my job in corporate I was']\n",
      "number of tokens: 3501\n",
      "----\n",
      "youtube_id: zsiFvhR1ekI, number of sents: 267\n",
      "first sents['back in February of this past year I', 'quit my first job out of college in', 'Investment Banking after working', 'full-time for only six months in the', 'months leading up to February 27 2022 I']\n",
      "number of tokens: 1799\n",
      "----\n",
      "TRANSCRIPT MISSING FOR VPQaE6TJGaU\n",
      "youtube_id: EHL67vXNuVg, number of sents: 1164\n",
      "first sents['hi guys if you came to this video um', \"you're wondering why I'm here I'm about\", 'to share a lot of things on this podcast', 'SLV video but yeah I just want this to', 'be a very casual conversation']\n",
      "number of tokens: 8632\n",
      "----\n",
      "youtube_id: lEGzPWb-oFw, number of sents: 465\n",
      "first sents['so one of my favorite stories of how', 'hard we saw a woman was actually like a', 'really sad weekend work story I remember', 'waking up really hungover on a Sunday', 'morning because I got out the night']\n",
      "number of tokens: 3264\n",
      "----\n",
      "youtube_id: qnXKRvza57s, number of sents: 309\n",
      "first sents[\"so as you read about the title I've left\", 'my job as a cyber security analyst I', 'wanted to make this video to be fully', 'transparent with you guys on that', 'decision because you guys have been a']\n",
      "number of tokens: 2291\n",
      "----\n",
      "youtube_id: jyoJ5cdJ6rA, number of sents: 14\n",
      "first sents['I quit my job for three hundred thousand', 'dollars I was working a corporate job', 'for about forty thousand dollars a year', \"but I just didn't like working in\", 'corporate so I had to decide do I stay']\n",
      "number of tokens: 101\n",
      "----\n",
      "youtube_id: 2mk2F4iUQVs, number of sents: 431\n",
      "first sents['all right guys so as you can already', \"tell by the title of the video i've quit\", 'my first ever job in cyber security', \"there's a lot of things i want to cover\", \"in this video and i'm sure you guys will\"]\n",
      "number of tokens: 3107\n",
      "----\n",
      "youtube_id: vOEd8GMPqO4, number of sents: 286\n",
      "first sents['thank you', 'hi there', 'uh so this is actually my last day so', \"I'm just turning in my ID yeah yeah is\", 'there anything else I need to do']\n",
      "number of tokens: 1919\n",
      "----\n",
      "youtube_id: G8Tj-ULBX8A, number of sents: 441\n",
      "first sents['I quit my job about 3 months ago to', \"start my own business and because I've\", 'been talking about it so much naturally', \"I've been getting more and more\", \"questions so today I thought I'd talk\"]\n",
      "number of tokens: 3412\n",
      "----\n",
      "youtube_id: hj3HgsVPVBE, number of sents: 221\n",
      "first sents['[Music]', '[Music]', 'oh', '[Music]', 'St']\n",
      "number of tokens: 1537\n",
      "----\n",
      "youtube_id: i-JHSBQJcHo, number of sents: 228\n",
      "first sents[\"what's up guys welcome back to my\", 'channel since i started this channel', 'with my first video back in september', 'last year i have been shocked and', 'grateful for the immense amount of']\n",
      "number of tokens: 1617\n",
      "----\n",
      "youtube_id: GtLFzvLSP8g, number of sents: 540\n",
      "first sents['hey friends welcome back to my channel', \"this is sandra and today's video is\", 'going to be sharing my experience all', 'about my new job as a security analyst', 'all right so if you guys have been']\n",
      "number of tokens: 3939\n",
      "----\n",
      "TRANSCRIPT MISSING FOR oxXQQEethiY\n",
      "youtube_id: Olbwq6KJ31U, number of sents: 15\n",
      "first sents[\"they don't like it when you leave how\", 'would you describe that for for Goldman', 'you heard right so that actually when I', 'was there they went through a transition', 'of trying to make it a little bit more']\n",
      "number of tokens: 111\n",
      "----\n",
      "youtube_id: 0To5Uci9S_o, number of sents: 13\n",
      "first sents['so today is officially my last day at', 'work', 'visually joining my', 'farewell call are we also out', 'I was hoping that I could also go to the']\n",
      "number of tokens: 74\n",
      "----\n",
      "youtube_id: -ByZCUVI92I, number of sents: 175\n",
      "first sents[\"I can't believe I'm making this video I\", 'think the time has come the time of', 'probably', 'telling my followers here and everyone', 'the behind the scenes of what really']\n",
      "number of tokens: 1299\n",
      "----\n",
      "youtube_id: VSGmjNiem-E, number of sents: 24\n",
      "first sents['so you offered an awesome job however', 'the salary is noticeably lower than you', 'expected the worst thing you can do is', 'to immediately accept and the second', 'worst is to flat out decline the better']\n",
      "number of tokens: 162\n",
      "----\n",
      "youtube_id: eG-ieMg-X18, number of sents: 140\n",
      "first sents[\"all right it's official i quit my big\", 'four job', 'unlike most other people i quit in less', \"than a year and honestly it wasn't even\", 'a hard decision to make so i broke this']\n",
      "number of tokens: 1017\n",
      "----\n",
      "youtube_id: ycUHBDWgXOQ, number of sents: 590\n",
      "first sents[\"and it is probably 11 o'clock at night\", \"here in Chicago and I'm here to let\", \"y'all know that I just quit my full-time\", 'job I am no longer with the marketing', \"agency that I'm currently at after four\"]\n",
      "number of tokens: 4417\n",
      "----\n",
      "youtube_id: c9R02XJffL8, number of sents: 285\n",
      "first sents['hey guys welcome back to my YouTube', 'channel okay guys so um tomorrow is', 'going to be my last day um with my', 'current employer and yeah for those of', 'you who have been following my channel']\n",
      "number of tokens: 2231\n",
      "----\n",
      "youtube_id: L_mSpj1KOu8, number of sents: 17\n",
      "first sents['this is what I did to break free from my', 'job and start working for myself one day', 'I found myself staring at the office', 'wall in my soul-sucking spreadsheet job', 'wondering how I could possibly continue']\n",
      "number of tokens: 115\n",
      "----\n",
      "youtube_id: FmjPtdP0Pv0, number of sents: 307\n",
      "first sents[\"i'm gonna do something i don't usually\", \"do on this channel and i'm gonna get\", 'into some personal anecdotes so a few', 'months ago i quit a data science job and', \"this isn't the first time that i've done\"]\n",
      "number of tokens: 2027\n",
      "----\n",
      "youtube_id: zfz-VrPNIYI, number of sents: 114\n",
      "first sents['after including my job at big4 I was', 'getting questions like how did you', 'resign what was the conversation like', 'who did you reach out to First and what', \"was your approach if you're planning to\"]\n",
      "number of tokens: 833\n",
      "----\n",
      "youtube_id: AoIqCmr6BJk, number of sents: 8\n",
      "first sents['one day until i give my two week notice', 'eight hours until i put my two week', 'notice in five hours until i put my two', 'weeks notice in', 'three hours']\n",
      "number of tokens: 42\n",
      "----\n",
      "youtube_id: nr3uRGNGivE, number of sents: 12\n",
      "first sents['this is how much i made as a corporate', 'lawyer my first year out of law school i', 'was making 180 000 as the base salary', 'second year was 190 000 the third year', 'was 220 000 plus if you work the number']\n",
      "number of tokens: 95\n",
      "----\n",
      "youtube_id: w3gmUMwix8I, number of sents: 28\n",
      "first sents[\"if you're looking around for an\", \"entry-level job I'm willing to Gander\", \"you're already pretty annoyed because\", 'entry-level jobs these days now require', 'five years of work experience how am I']\n",
      "number of tokens: 191\n",
      "----\n",
      "youtube_id: VRgxh4UetOM, number of sents: 12\n",
      "first sents[\"have you ever wondered what it's like to\", \"work at google well here's the new york\", 'office', 'they have recreation rooms for their', 'employees loads of options for food and']\n",
      "number of tokens: 75\n",
      "----\n",
      "youtube_id: IZCfP13twxk, number of sents: 290\n",
      "first sents['Google employees and relationships that', 'start on Tinder have one thing in common', \"on average they don't last very long\", 'while employees in America stay with a', 'company for 4.1 years on average a']\n",
      "number of tokens: 1947\n",
      "----\n",
      "TRANSCRIPT MISSING FOR VjDJC2rkrzE\n",
      "youtube_id: 8MwjHCYKZGQ, number of sents: 139\n",
      "first sents['6 years ago I thought I found the', 'perfect career data analytics was', 'exciting promising and full of', 'opportunities I built a five figure', 'business teaching people how to landar']\n",
      "number of tokens: 970\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "### CHECK FUNCTIONALIZED VERSION\n",
    "i = 0\n",
    "for curr_key in transcript_dict.keys():\n",
    "    try:\n",
    "        print(f\"youtube_id: {curr_key}, number of sents: {len(transcript_dict[curr_key]['sents'])}\")\n",
    "        print(f\"first sents{transcript_dict[curr_key]['sents'][:5]}\")\n",
    "        print(f\"number of tokens: {len(transcript_dict[curr_key]['joined'].split())}\")\n",
    "        print(\"----\")\n",
    "    except:\n",
    "        print(f\"TRANSCRIPT MISSING FOR {curr_key}\") \n",
    "    i += 1 \n",
    "\n",
    "    if i == 50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c13fa33f-ce6b-4808-a681-503f19891dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcripts_json_outname = f\"{data_folder}/02a_transcripts.json\"\n",
    "with open(transcripts_json_outname, 'w', encoding='utf-8') as f:\n",
    "    json.dump(transcript_dict, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "90d618ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ADDED TO FUNCTION ALREADY\n",
    "# # with open(in_filepath, 'r') as f:\n",
    "# #     data = json.load(f)\n",
    "\n",
    "# #### WORKING - MOVE TO 02?\n",
    "# ##Get transcripts using get_transcript() of YoutubeTranscriptApi for each transcript\n",
    "# ### Get as dictionary of dictionary: youtube_id : {}\n",
    "\n",
    "# transcript_dict = {}\n",
    "# yt_api = YouTubeTranscriptApi()\n",
    "\n",
    "# for i, video_id in enumerate(input_df['video_id']):\n",
    "#     transcript_sents = None #create new list for individual sentences\n",
    "#     transcript_joined = None\n",
    "#     try:\n",
    "        \n",
    "#         curr_result = yt_api.get_transcript(video_id,languages=['en'])\n",
    "#         # print(curr_result)\n",
    "#         transcript_sents = [str(x['text']).replace(\"\\xa0\", \"\") for x in curr_result]\n",
    "#         transcript_joined = \" \".join(transcript_sents)\n",
    "#         print(f\"WORKING - {video_id}\")\n",
    "#         print(f\"FIRST SENTS FOR {video_id} TRANSCRIPT: {transcript_sents[:5]}\")\n",
    "        \n",
    "        \n",
    "#     except:\n",
    "#         print(f\"MISSING {video_id} - SKIPPED\")\n",
    "#     transcript_dict[video_id] = {\n",
    "#                                     \"joined\": transcript_joined,\n",
    "#                                     \"sents\": transcript_sents\n",
    "        \n",
    "#                                 }\n",
    "\n",
    "#     if i == 5:\n",
    "#         break\n",
    "#     # if i % 50 == 0:\n",
    "#     #     print(f\"PROCESSED {i} TRANSCRIPTS\")\n",
    "#     #     print(f\"LENGTH OF DICTS\")\n",
    "\n",
    "# # print(len(transcript_dict))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1a3adff4-5c1f-4c9b-ab58-dd61aa410965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for curr_key in transcript_dict.keys():\n",
    "#     try:\n",
    "#         print(f\"youtube_id: {curr_key}, number of sents: {len(transcript_dict[curr_key]['sents'])}\")\n",
    "#         print(f\"first sents{transcript_dict[curr_key]['sents'][:5]}\")\n",
    "#         print(f\"number of tokens: {len(transcript_dict[curr_key]['joined'].split())}\")\n",
    "#         print(\"----\")\n",
    "#     except:\n",
    "#         print(f\"TRANSCRIPT MISSING FOR {curr_key}\") \n",
    "\n",
    "# print(transcript_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf5c950-bf5d-4059-8394-002a786928b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO DO: verify transcript dict and save to data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "41c91132-bacf-4bfb-92b2-f3c128d04615",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d60847ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR WITH None - SKIPPED\n",
      "ERROR WITH None - SKIPPED\n",
      "ERROR WITH None - SKIPPED\n",
      "ERROR WITH None - SKIPPED\n",
      "PROCESSED 50 TRANSCRIPTS\n",
      "{\n",
      "    \"current_job\": \"\",\n",
      "    \"previous_job\": \"\",\n",
      "    \"quit_reason_main\": \"\",\n",
      "    \"emotions_list\": [],\n",
      "    \"emotions_dict\": {}\n",
      "}\n",
      "_____\n",
      "ERROR WITH None - SKIPPED\n",
      "ERROR WITH None - SKIPPED\n",
      "ERROR WITH None - SKIPPED\n",
      "ERROR WITH None - SKIPPED\n",
      "ERROR WITH None - SKIPPED\n",
      "ERROR WITH None - SKIPPED\n",
      "ERROR WITH None - SKIPPED\n",
      "ERROR WITH None - SKIPPED\n",
      "ERROR WITH None - SKIPPED\n",
      "ERROR WITH None - SKIPPED\n",
      "ERROR WITH None - SKIPPED\n",
      "ERROR WITH None - SKIPPED\n",
      "ERROR WITH None - SKIPPED\n",
      "ERROR WITH None - SKIPPED\n",
      "ERROR WITH None - SKIPPED\n",
      "ERROR WITH None - SKIPPED\n",
      "ERROR WITH None - SKIPPED\n",
      "ERROR WITH None - SKIPPED\n",
      "ERROR WITH None - SKIPPED\n",
      "ERROR WITH None - SKIPPED\n",
      "ERROR WITH None - SKIPPED\n",
      "ERROR WITH None - SKIPPED\n",
      "ERROR WITH None - SKIPPED\n",
      "ERROR WITH None - SKIPPED\n",
      "ERROR WITH None - SKIPPED\n",
      "ERROR WITH None - SKIPPED\n",
      "ERROR WITH None - SKIPPED\n",
      "ERROR WITH None - SKIPPED\n",
      "ERROR WITH None - SKIPPED\n",
      "ERROR WITH None - SKIPPED\n",
      "ERROR WITH None - SKIPPED\n",
      "ERROR WITH None - SKIPPED\n",
      "ERROR WITH None - SKIPPED\n",
      "ERROR WITH None - SKIPPED\n",
      "ERROR WITH None - SKIPPED\n",
      "ERROR WITH None - SKIPPED\n",
      "ERROR WITH None - SKIPPED\n",
      "ERROR WITH None - SKIPPED\n",
      "ERROR WITH None - SKIPPED\n",
      "ERROR WITH None - SKIPPED\n",
      "ERROR WITH None - SKIPPED\n",
      "ERROR WITH [Music] i'm literally salivating oh my gosh um i don't think i'm prepared for how spicy this is gonna be but uh i'm excited i'm excited for the uh three times spicy food [Music] this is so rad oh gosh i'm just celebrating though okay i just licked it okay let's go my mouth is on fire okay let's get through this quickly first yum all right spicy but yeah oh my gosh my face is going red once you get used to it is better my lips are so red hey everyone so i just finished filming a tick tock for a sydney-based apparel company called gray lines which is why my hair is like fully up in a huge bun and then i chucked a bow in my hair because i was kind of going for a cutesy look tonight is actually my farewell dinner with my company tomorrow is my last day but since most people have tomorrow off as a half day we're going to do fl dinner tonight and as some of you may or may not know i'm actually quitting my full-time job to focus on studying so i'm doing a master's in teaching primary school so i'm aiming to be a primary school teacher and that is going to finish in about a year and a bit so i should be graduated by mid 2023 which is kind of scary because that means i could be handling 20 kids 30 kids on my own by mid and next year but it's very very exciting i'm loving the courses so far but with prak coming and just the trouble of that coordinating with my current work i just decided to concentrate on studying for now and i will start looking for casual teaching jobs probably mid this year when i can apply for my teacher's number and can start picking up casual work so i just had my last official team meeting and my manager gave me such a lovely speech about me leaving she was the manager i worked with originally when i first started the company and then she moved to another team and then i was promoted into her team again so we got to work together again and such a bittersweet moment when you leave a workplace where you love the people but the work doesn't speak to you i'm excited for my new chapter and i am excited for what i'm going to do but i'm sad to be leaving behind such an amazing group of people that i work with on the day today and yeah just feeling a bit sad because i'm definitely going to miss everyone [Music] so in the age of coped we have transitioned to digital farewell cards and i just removed mine from my team from both my current team and my old team it's so cute i got a digital card from everyone which has gifts in it because they know i love sending good gifts and i printed it just like a physical copy as well because i wanted something like i wanted a physical keepsake i just got a call from one of the girls on my team and she said like a you know one-on-one goodbye to me and that was really really lovely it's just such a bittersweet moment to say goodbye to a team that i really really love so yeah but i'm sure that we'll be able to catch up in the near future once everything was kind of blown over because you know we've i think we've developed quite a strong bond through work and just through personal like interactions so i'm sure that we'll be able to catch up and continue to be friends beyond this and i'm also expecting a couple packages today from work so we'll see when that gets in [Music] oh that's so cute smells so nice and a little card inside as well [Music] i've just gone downstairs to pick up a another parcel that was sent to it's from black velvet cake and we've sent these to other members of our team for their birthday dessert and i've heard really really good things about them i feel like that's a cookies and cream maybe like that's vanilla chocolate that one's probably caramel another type of vanilla i'm not not sure maybe that one's cherry soup but i'm super super excited to dig into these i think there was a card as well so sweet i can't wait to have these [Music] knowing that the bar was coming back was a fabulous feeling my name is 28 years old [Music] hello [Music] [Music] this is what i got for my secret santa and we all a bunch of our friends all put up like a wish list of what we wanted and i put mine up and it's been so long i've completely forgotten what i wanted so let's have a look that is my own hand cream looks like i have a coach um coaching sephora collab tea rose eyebrows which is really nice this will be really really interesting really relaxing [Music] makeup set this will be really good for traveling because you have everything you have eyeliner mascara highlighter eyeshadows and blush which is pretty much a full face let's open this up and see what's inside so nicely wrapped again [Music] gel eyeliner and this is the palette oh my gosh and these are the swatches so that's the highlighter which looks really nice and natural is that base color there that blended really well in my skin a crease color a glitter and like a darker color which you can use on your waterline or deepen the outer v and then that is the blush these all went on really really smoothly so i'm really excited to try them out i'm actually going out tonight so i might try all this stuff on my face and see how it goes coming to asian grocers definitely makes me feel more like at home ah this is where i spend so much of my childhood following my mom around in the asian grocers oh my god i haven't had those in ages childhood memories treating ourselves so today is the first day after getting a booster i'm just woken up and my arm is so sore i think it's also because it's the side that i sleep on my own is really really soft and i'm trying to get a headache the rest of my body is more so sore but that's from going to the gym doubly so definitely going to take too easy um ah keys comes out today which is so exciting uh i pre-ordered it a while back so i'm gonna go collect it today and i'm also gonna stock up on some skincare cut a couple errands to run nothing like strenuous so fingers crossed that i'll be fine to do those up last time i got the vaccine i was saw everywhere like clothes hurt my skin was so sensitive so hopefully i don't get that this time so the nurses took my bloods and i'm waiting for the results to come back so i can go home i'm so sleepy this booster has really taken it out of me i can't wait to go home and sleep in my bed after three four hours at the hospital i'm finally cleared of everything so i can go home now i'm super super tired oh there's an update laughter [Music] [Music] hi everyone today is january 31st which means it's the last day of my january vlog coincidentally today is also luna new year's eve which is the day where we have our big feast as a family and invite a couple family friends over to share as well my dad usually makes the lunar new year fees and he's confirmed the menu for tonight and sounds pretty good so i'm definitely looking forward to that and we will also have a couple family friends over so that's kind of exciting what else has been happening yeah to prepare for lunar new year we've been really cleaning up the house we've put a lot of things in storage we've thrown a lot of things out that should probably should have been thrown out like ages ago and i was literally just on my hands and knees wiping down the door and the door frame to welcome in chai sun which is the chinese like deity for fortune and prosperity funnily enough last night i also dreamt of the number eight which in chinese omens it's a good number like it's a number for fortune and prosperity so he's hoping that 2022 will be a prosperous year for me um and also i have two skin care products that are like finishing up like that i finished today pretty much so i'll have to use new ones tomorrow which is in the new year and in the lunar new year you're supposed to use new things to signify a new beginning so i feel like all i'm not usually a superstitious person but i feel like all things are pointing towards a great 2022 so he's hoping that throughout the vlogs this year that you'll be able to see a good 2022 for me um what else has been happening yeah we just have been doing we've just been doing a huge spring clean normally i feel like we haven't done this as a family but this year my mum for whatever reason has been in a super super like cleaning mood so we've cleared out so much stuff especially from my room because my room has been used as storage for a lot of things so i'm glad that i have more space in my room now it's a lot less cluttered which is good i finally have a chance to put something like put a table down as my vanity which will be really nice i don't want to contin to have to keep doing makeup on my like work uh work death because it'll just get it dirty and i don't want to get you know like foundation and powders and stuff on my electronics so that's good i think i'll probably look into that in the next couple of months and then i'll have a new vanity space and more storage in my room which is so exciting and yeah i'll try to vlog a bit of the dinner tonight to show you guys what our lunar new year face looks like but yeah i think i'll take the rest of the day pretty chill i'll probably game a little bit more today and then welcome in the new year [Music] [Music] [Music] [Music] you - SKIPPED\n",
      "ERROR WITH was it optimistic yes was it like a dream in the sky maybe if you're watching this video you're more than likely a new youtuber or you are a youtuber that finally decided to take your content seriously enough to want to kind of generate some kind of income from it i'm the second i'm the latter myself so i decided i'm gonna take my youtube way more seriously because i want to generate some kind of income from my youtube i'm on the journey myself and i'll be sharing lessons that i've learned from my online marketing classes from all the other youtubers and content creators out there in the world that share their knowledge about how to grow and maintain a growth as you come up in your youtube game one second are we good okay in case you're new here i'm maria fade the maria fadi on all social media platforms and on this channel we talk about obviously everything fashion and content creation and everything in between that and today i'll be talking about some strategies that i've been implementing to grow my youtube channel while i get ready and dressed to go visit my mom later today for lunch with my fiance so we're going to be getting dressed i wanted to create a side income on youtube by q4 of 2022. was it optimistic yes was it like a dream in the sky maybe but if you don't dream big like what's the point right my goal was q4 2022 we're about to approach it so i don't know if i'm going to reach that goal of meeting it before but the goal is to create an income by q4 which is end of the year the last video talks about three different techniques that i've seen other youtubers implement on their channel that's causing them to have some growth future shorts has been my biggest follower growth posting of course in the past three to four weeks last time i checked it which was yesterday i have 262 so that was 88 followers in a month now to those accounts that have millions or thousands of hundreds of thousands of followers this might not be that much to them because they probably get that like in a second but for a new youtube account which is what we both have because if you're watching this that's what we have for a new youtube account having 88 followers in one month is a huge growth that growth is what i saw from being consistent in posting on my youtube shorts in the past three to four weeks i'll be remiss insane i am so grateful for all 88 new followers of you joining me thanks for being here and joining this community youtube charts brought in a bit of followers for me and a little bit of engagement as well but it's still not counting to the almighty public watch hours and i don't know if you know this but if you go onto your youtube analytic page you'll say the watch hours that you get from your shorts do not count towards the watch hours that you have for your youtube account like in general towards the you know their creator fund i don't know if it's even called a creator front i think it's just the youtube monetization fund whatever i call a creator fund because i'm from tiktok i'm a tick-tock person mainly that's my main platform if you hear me say creator fund you know what i mean it's monetization here on youtube and now youtube shorts monetization just regular monetization so yes there is a youtube shorts fun you have to be invited i haven't been invited yet we're hoping that comes through but my main goal is to be on the youtube like the regular youtube creator fund that's my main goal that's what i want that's what i hope for myself that's really what i want but what really short is doing for me is creating credibility for me on the platform with both the algorithm and my audience both new and old now you're going to come with me as i get ready as i mentioned because i have to wash my face and do my little bit of makeup and get dressed so let's go do that while we talk i'm gonna now make all of this match the fit i want to be done getting ready by two upload all of the files that i recorded today onto my hard drive before we leave so let's do that now this brings up to the idea that if you have tons of followers you more than likely will get more followers this is really known as the snowball effect snowball effect in relation to marketing and youtube specifically is the idea that if you put out enough video and of course your audience starts off slow just like with everybody else no one knows who you are so of course the growth and the momentum are slow it mimics this concept of a snowball rolling down a hill but eventually gains more momentum gains more speed gains more growth the snowball gets bigger as it rolls down this hill so it's moving faster quicker it mimics itself in the youtube world in the world of content creation as audience growth as brand deals as retention as exposure stuff that we all want comes with putting in that work at the end where that snowball is a little bit small and then rolling it down the hill with the work that we put in and it's faster and quicker as we move along let's get back to the shorts idea and i'm going to be steaming my clothes as i talk about the shorts idea so i hope it's not too loud an amazing creator who is actually a mutual of mine on take talk like i said that is my main platform her name is robbie jim and i don't know if you've noticed but she's been doing an amazing work not just growing her channel but just in fashion in general you can see that her account is primarily shorts shorts are her biggest growth she's had tons of shorts go viral millions and millions of views on a million tons and tons of her shorts and as i'm talking to you right now with this steamer in my face she has over 400 000 followers on youtube from outside of perspective because i am an outsider i don't know what she is doing on the inside or you know her background per se but from an outsider's perspective looking at her channel most of her growth came from youtube shorts because there were so many other videos that went viral i have no doubt she's probably in the youtube shorts fund she seems like she would be the youtube shorts fund if she's not it's a big opportunity missed by youtube she should be in it and again i didn't verify this with her so i don't know she probably is but if she's not i would be shocked like literally beside myself because she also has long form content on her page i'm sure she's also getting the regular monetization funds at this point too so again i'm not here to count her money i'm not here to counter coins i'm just saying using that as a case study like i talked about in my other video using that as a case study to see what people are doing to grow their channel where they could be making that money that is probably two means or two avenues of growth for her and that's probably two ways that she's actually raking in a little bit of money right now which i'm sure is doing really well for her oh shorts in itself could be very lucrative for you it doesn't just have to be the regular monetization fund which we all know about and wanted to join but shorts is another avenue that you could hedge your bets with and that's really what i'm doing hedging my bets with having shorts and long form content like you see with robbie like you see with me while my battery charges i decided let me just sit down and have a conversation with you as that does what it needs to do so like i said robbie's growth isn't like mine and i don't expect it to she took youtube shorts seriously in the 2021 period where it really i feel gave her a really good boom that was kind of like that coveted period and i'm not saying that she wouldn't have been successful without it i'm just saying that that time was prime short time and she really took it seriously and she really got her content out there and it's amazing content like i said but i didn't do that i did not take it seriously that much then if you look back on my channel you can see that a lot of the videos that i posted at that time actually probably maybe like in like maybe once every two weeks on average so i didn't take it seriously like she took it seriously she's a white woman i'm a black woman and i'm not discrediting from her work not at all i'm just saying that there are different factors that you have to think about while you're growing your channel that is just the reality of it but i saw the potential of that short idea that she implemented and i was like i'm kind of missing out here because i did this on on my tick tock why wouldn't i bring this over here like it made no sense so looking at my analytics as of right now it could change by the time i actually get more screenshots so i'm just saying as of right now oh of the 88 new followers that i had in the time period for my last check-in to this check-in minus the last seven days or so that i kind of i'm recording this right now but of the 88 followers that i gained in that past month period 59 of them came from shorts which is like what is that like two-thirds pretty much of the growth came from shorts for me and i'm glad to obviously have the other growth come from my videos that means people are seeing it i do post a lot more shorts so of course you know people see that way more so do without what you will seeing that you could really kind of get some traction with shorts i would say try the shorts challenge and i want to hear from you especially people are in this challenge with me how has it been going for you are you going to be implementing shorts is shorts working for you is it not what are some things that you think or you would like me to show you about shorts that's gonna work for me that maybe you think might not be working for you let me know in the comments but overall i think you should give shorts a try i think you should add it as part of your content strategy for youtube in general most of our content is going towards short form content at this point so it doesn't harm you it doesn't hurt you to have that as an evergreen content moving forward when people search for you or any kind of content related to your topic and based on all of that i want you all who are in this challenge to go and create seven to eight pieces of content in your niche thing what kind of favors would you like to watch and what you want to put out there i want you to create seven to eight pieces of content and upload it every two days and i want you to try i want you to create some kind of video whether it's a trend whether it's just something you've really wanted to show people something that you're passionate about it doesn't matter how cringy it is just make some videos seven to eight videos in your niche thing in the niche world that you call your own world and post it every two days for the next two weeks you can't really see a full result in like a week or so you need two weeks to even longer to kind of see the benefits of your work so we're gonna have a check-in in two weeks from now to see like i said we're doing bi-weekly check-ins in two weeks from now to see your growth let me know if the youtube shorts method if you're going to be implementing it how you're going to be implementing it if you're going to be joining this two-week challenge which i'm gonna be doing as well i'm gonna be posting at least twice well at least every two days i'll be posting a short long form short form different style of like formats and just trying out and coming back in another two weeks with another plan that i'll probably be implementing on top of that which is these behind-the-scene content and the whole analytics dive in which is part of my like i told you part of the strategy and until next time bye my loves oh my god the light here is so good [Music] - SKIPPED\n",
      "ERROR WITH None - SKIPPED\n",
      "ERROR WITH None - SKIPPED\n",
      "ERROR WITH None - SKIPPED\n",
      "ERROR WITH None - SKIPPED\n",
      "ERROR WITH None - SKIPPED\n",
      "ERROR WITH None - SKIPPED\n",
      "ERROR WITH None - SKIPPED\n",
      "ERROR WITH None - SKIPPED\n",
      "ERROR WITH None - SKIPPED\n",
      "ERROR WITH None - SKIPPED\n",
      "ERROR WITH None - SKIPPED\n",
      "ERROR WITH None - SKIPPED\n",
      "ERROR WITH None - SKIPPED\n",
      "ERROR WITH None - SKIPPED\n",
      "ERROR WITH None - SKIPPED\n",
      "ERROR WITH None - SKIPPED\n",
      "ERROR WITH None - SKIPPED\n",
      "ERROR WITH None - SKIPPED\n",
      "ERROR WITH None - SKIPPED\n",
      "ERROR WITH None - SKIPPED\n",
      "ERROR WITH None - SKIPPED\n",
      "ERROR WITH None - SKIPPED\n",
      "ERROR WITH None - SKIPPED\n",
      "ERROR WITH None - SKIPPED\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>current_job</th>\n",
       "      <th>previous_job</th>\n",
       "      <th>quit_reason_main</th>\n",
       "      <th>emotions_list</th>\n",
       "      <th>emotions_dict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bVJfQAe-UP4</td>\n",
       "      <td>analytics manager</td>\n",
       "      <td>data analyst at AmerisourceBergen</td>\n",
       "      <td>The speaker quit their job to pursue growing o...</td>\n",
       "      <td>surreal_excited_nervous_grateful</td>\n",
       "      <td>{'surreal': 'The speaker feels surreal about t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-ayEjbg0ZEc</td>\n",
       "      <td>indie hacker</td>\n",
       "      <td>financial analyst</td>\n",
       "      <td>The speaker quit their job as a financial anal...</td>\n",
       "      <td>relief_excitement_concern</td>\n",
       "      <td>{'relief': 'The speaker felt relieved after ga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C9h0uhjdsOI</td>\n",
       "      <td>content creator on YouTube</td>\n",
       "      <td>data analyst</td>\n",
       "      <td>The primary reason for quitting was the increa...</td>\n",
       "      <td>relief_determination_anxiety</td>\n",
       "      <td>{'relief': 'The speaker feels relieved to esca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sd5F1uR3tvA</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>joy_excitement</td>\n",
       "      <td>{'joy': 'The speaker feels joy from the vibran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M8md7_gyBy4</td>\n",
       "      <td>content creator</td>\n",
       "      <td>data analyst</td>\n",
       "      <td>The speaker quit their job because they wanted...</td>\n",
       "      <td>scared_confident_determined</td>\n",
       "      <td>{'scared': 'The speaker feels scared about lea...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      video_id                 current_job                       previous_job  \\\n",
       "0  bVJfQAe-UP4           analytics manager  data analyst at AmerisourceBergen   \n",
       "1  -ayEjbg0ZEc                indie hacker                  financial analyst   \n",
       "2  C9h0uhjdsOI  content creator on YouTube                       data analyst   \n",
       "3  sd5F1uR3tvA                                                                  \n",
       "4  M8md7_gyBy4             content creator                       data analyst   \n",
       "\n",
       "                                    quit_reason_main  \\\n",
       "0  The speaker quit their job to pursue growing o...   \n",
       "1  The speaker quit their job as a financial anal...   \n",
       "2  The primary reason for quitting was the increa...   \n",
       "3                                                      \n",
       "4  The speaker quit their job because they wanted...   \n",
       "\n",
       "                      emotions_list  \\\n",
       "0  surreal_excited_nervous_grateful   \n",
       "1         relief_excitement_concern   \n",
       "2      relief_determination_anxiety   \n",
       "3                    joy_excitement   \n",
       "4       scared_confident_determined   \n",
       "\n",
       "                                       emotions_dict  \n",
       "0  {'surreal': 'The speaker feels surreal about t...  \n",
       "1  {'relief': 'The speaker felt relieved after ga...  \n",
       "2  {'relief': 'The speaker feels relieved to esca...  \n",
       "3  {'joy': 'The speaker feels joy from the vibran...  \n",
       "4  {'scared': 'The speaker feels scared about lea...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### NEW EXAMPLE - need to fix the limits and billing on OPENAI website: https://platform.openai.com/settings/organization/billing/overview \n",
    "#### Source: https://cookbook.openai.com/examples/batch_processing\n",
    "categorize_system_prompt = \"\"\"\n",
    "                You are a text analysis assistant, skilled in identifying key arguments and summarizing text transcripts with brevity and neutrality.\n",
    "                \n",
    "                The following transcript is from a YoutTbe video concerning their decision to quit their job. \n",
    "                Please identify the following items from the text: current job, former job, 1-3 primary reasons for them quitting their job, and 1-3 emotions that the speaker is feeling. \n",
    "                Please output a json object containing the following information:\n",
    "\n",
    "                {\n",
    "                    current_job: string // the current job of the speaker\n",
    "                    previous_job: string // the job that the speaker most recently quit\n",
    "                    quit_reason_main: string // 1-sentence summary of the primary reason that the speaker quit their job\n",
    "                    emotions_list: string [] // list of emotions that the speaker has displayed \n",
    "                    emotions_dict: dictionary {emotion_name: cause_of_emotion} // dictionary where the key is an emotion that the speaker exhibits, and the value is a 1-sentence summary of why the speaker feels this emotion. \n",
    "                }\n",
    "\n",
    "                quit_reason_main should include the main reason that the speaker provided for why they quit their job - be specific as possible.\n",
    "                emotions_list can include multiple emotions, but try to keep it to under 3-5 and keep them simple and only use lowercase letters.\n",
    "                emotions_dict can include multiple emotions, but try to keep it to under 3-5. For each key-value pair in emotions_dict, the key should be simple and only use lower case letters. \n",
    "                \n",
    "                \"\"\"\n",
    "\n",
    "def get_categories(description):\n",
    "    response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.1,\n",
    "    # This is to enable JSON mode, making sure responses are valid json objects\n",
    "    response_format={ \n",
    "        \"type\": \"json_object\"\n",
    "    },\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": categorize_system_prompt\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": description\n",
    "        }\n",
    "    ],\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def create_qa_dict(input_dict):\n",
    "    \"\"\"\n",
    "    Creates a qa dict with the transcripts\n",
    "\n",
    "    Assume get_categories is the main function to reference chatGPT\n",
    "    \"\"\"\n",
    "    ### Create qa_dict \n",
    "    qa_dict = {}\n",
    "    for i, video_id in enumerate(input_dict.keys()):\n",
    "        curr_transcript = transcript_dict[video_id]['joined']\n",
    "        try:\n",
    "            result = get_categories(curr_transcript)\n",
    "            qa_dict[video_id] = result\n",
    "        except:\n",
    "            print(f\"ERROR WITH {curr_transcript} - SKIPPED\")\n",
    "        \n",
    "        # print(result)\n",
    "        # print(\"---\")\n",
    "        if i % 50 == 0:\n",
    "            print(f\"PROCESSED {i} TRANSCRIPTS\")\n",
    "            print(result)\n",
    "            print(\"_____\")\n",
    "\n",
    "    #### WORKING - Make qa_dict into flat JSOn to create the dataframe\n",
    "    qa_dict_final = {\n",
    "        \"video_id\": [],\n",
    "        \"current_job\": [],\n",
    "        \"previous_job\": [],\n",
    "        \"quit_reason_main\":[],\n",
    "        \"emotions_list\":[],\n",
    "        \"emotions_dict\":[],\n",
    "    \n",
    "    }\n",
    "    # ÷\n",
    "    for video_id, results in qa_dict.items():\n",
    "        qa_dict_final['video_id'].append(video_id)\n",
    "        # print(results)\n",
    "        results_json = json.loads(results)\n",
    "        for curr_col, curr_val in results_json.items():\n",
    "            qa_dict_final[curr_col].append(curr_val)\n",
    "    qa_df = pd.DataFrame(qa_dict_final)  \n",
    "    qa_df['emotions_list']= qa_df['emotions_list'].str.join(\"_\") \n",
    "    \n",
    "    return qa_df\n",
    "\n",
    "qa_df=create_qa_dict(transcript_dict)\n",
    "qa_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7d4aae66-1c73-4b09-a76e-21aa44846a51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>current_job</th>\n",
       "      <th>previous_job</th>\n",
       "      <th>quit_reason_main</th>\n",
       "      <th>emotions_list</th>\n",
       "      <th>emotions_dict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bVJfQAe-UP4</td>\n",
       "      <td>analytics manager</td>\n",
       "      <td>data analyst at AmerisourceBergen</td>\n",
       "      <td>The speaker quit their job to pursue growing o...</td>\n",
       "      <td>surreal_excited_nervous_grateful</td>\n",
       "      <td>{'surreal': 'The speaker feels surreal about t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-ayEjbg0ZEc</td>\n",
       "      <td>indie hacker</td>\n",
       "      <td>financial analyst</td>\n",
       "      <td>The speaker quit their job as a financial anal...</td>\n",
       "      <td>relief_excitement_concern</td>\n",
       "      <td>{'relief': 'The speaker felt relieved after ga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C9h0uhjdsOI</td>\n",
       "      <td>content creator on YouTube</td>\n",
       "      <td>data analyst</td>\n",
       "      <td>The primary reason for quitting was the increa...</td>\n",
       "      <td>relief_determination_anxiety</td>\n",
       "      <td>{'relief': 'The speaker feels relieved to esca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sd5F1uR3tvA</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>joy_excitement</td>\n",
       "      <td>{'joy': 'The speaker feels joy from the vibran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M8md7_gyBy4</td>\n",
       "      <td>content creator</td>\n",
       "      <td>data analyst</td>\n",
       "      <td>The speaker quit their job because they wanted...</td>\n",
       "      <td>scared_confident_determined</td>\n",
       "      <td>{'scared': 'The speaker feels scared about lea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>GAfbM5gONHc</td>\n",
       "      <td>full-time YouTuber</td>\n",
       "      <td>ad agency employee</td>\n",
       "      <td>The speaker quit their job to pursue YouTube f...</td>\n",
       "      <td>excitement_nervousness_gratitude</td>\n",
       "      <td>{'excitement': 'The speaker feels excited abou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443</th>\n",
       "      <td>QkXfWALFQ68</td>\n",
       "      <td>English teacher</td>\n",
       "      <td>not specified</td>\n",
       "      <td>The speaker felt overwhelmed by their workload...</td>\n",
       "      <td>overwhelmed_excited_hopeful</td>\n",
       "      <td>{'overwhelmed': 'The speaker felt overwhelmed ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>0m7tN6riipc</td>\n",
       "      <td>English teacher</td>\n",
       "      <td>not specified</td>\n",
       "      <td>The speaker felt overwhelmed by their workload...</td>\n",
       "      <td>overwhelmed_excited_hopeful</td>\n",
       "      <td>{'overwhelmed': 'The speaker felt overwhelmed ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>ejaY282wnCQ</td>\n",
       "      <td>Prime Minister of Canada</td>\n",
       "      <td>Governor of the Bank of England</td>\n",
       "      <td>Mark Carney is seeking a strong mandate to add...</td>\n",
       "      <td>determined_hopeful_confident</td>\n",
       "      <td>{'determined': 'Carney expresses a strong comm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>-NgKKDLdPhI</td>\n",
       "      <td>not specified</td>\n",
       "      <td>not specified</td>\n",
       "      <td>The speaker did not clearly articulate a speci...</td>\n",
       "      <td></td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>447 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        video_id                 current_job  \\\n",
       "0    bVJfQAe-UP4           analytics manager   \n",
       "1    -ayEjbg0ZEc                indie hacker   \n",
       "2    C9h0uhjdsOI  content creator on YouTube   \n",
       "3    sd5F1uR3tvA                               \n",
       "4    M8md7_gyBy4             content creator   \n",
       "..           ...                         ...   \n",
       "442  GAfbM5gONHc          full-time YouTuber   \n",
       "443  QkXfWALFQ68             English teacher   \n",
       "444  0m7tN6riipc             English teacher   \n",
       "445  ejaY282wnCQ    Prime Minister of Canada   \n",
       "446  -NgKKDLdPhI               not specified   \n",
       "\n",
       "                          previous_job  \\\n",
       "0    data analyst at AmerisourceBergen   \n",
       "1                    financial analyst   \n",
       "2                         data analyst   \n",
       "3                                        \n",
       "4                         data analyst   \n",
       "..                                 ...   \n",
       "442                 ad agency employee   \n",
       "443                      not specified   \n",
       "444                      not specified   \n",
       "445    Governor of the Bank of England   \n",
       "446                      not specified   \n",
       "\n",
       "                                      quit_reason_main  \\\n",
       "0    The speaker quit their job to pursue growing o...   \n",
       "1    The speaker quit their job as a financial anal...   \n",
       "2    The primary reason for quitting was the increa...   \n",
       "3                                                        \n",
       "4    The speaker quit their job because they wanted...   \n",
       "..                                                 ...   \n",
       "442  The speaker quit their job to pursue YouTube f...   \n",
       "443  The speaker felt overwhelmed by their workload...   \n",
       "444  The speaker felt overwhelmed by their workload...   \n",
       "445  Mark Carney is seeking a strong mandate to add...   \n",
       "446  The speaker did not clearly articulate a speci...   \n",
       "\n",
       "                        emotions_list  \\\n",
       "0    surreal_excited_nervous_grateful   \n",
       "1           relief_excitement_concern   \n",
       "2        relief_determination_anxiety   \n",
       "3                      joy_excitement   \n",
       "4         scared_confident_determined   \n",
       "..                                ...   \n",
       "442  excitement_nervousness_gratitude   \n",
       "443       overwhelmed_excited_hopeful   \n",
       "444       overwhelmed_excited_hopeful   \n",
       "445      determined_hopeful_confident   \n",
       "446                                     \n",
       "\n",
       "                                         emotions_dict  \n",
       "0    {'surreal': 'The speaker feels surreal about t...  \n",
       "1    {'relief': 'The speaker felt relieved after ga...  \n",
       "2    {'relief': 'The speaker feels relieved to esca...  \n",
       "3    {'joy': 'The speaker feels joy from the vibran...  \n",
       "4    {'scared': 'The speaker feels scared about lea...  \n",
       "..                                                 ...  \n",
       "442  {'excitement': 'The speaker feels excited abou...  \n",
       "443  {'overwhelmed': 'The speaker felt overwhelmed ...  \n",
       "444  {'overwhelmed': 'The speaker felt overwhelmed ...  \n",
       "445  {'determined': 'Carney expresses a strong comm...  \n",
       "446                                                 {}  \n",
       "\n",
       "[447 rows x 6 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1cc486d8-3f34-4b3b-ab1e-9ab23fcec3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_filepath_csv = f\"{data_folder}/02b_QA_results.csv\"\n",
    "qa_df.to_csv(out_filepath_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7dd46f9e-fb46-4a73-8210-f4a041efe367",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ADDED TO FUNCTIONS\n",
    "# #### WORKING - Make qa_dict into flat JSOn to create the dataframe\n",
    "\n",
    "# qa_dict_final = {\n",
    "#     \"video_id\": [],\n",
    "#     \"current_job\": [],\n",
    "#     \"previous_job\": [],\n",
    "#     \"quit_reason_main\":[],\n",
    "#     \"emotions_list\":[],\n",
    "#     \"emotions_dict\":[],\n",
    "\n",
    "# }\n",
    "\n",
    "# for video_id, results in qa_dict.items():\n",
    "#     qa_dict_final['video_id'].append(video_id)\n",
    "#     # print(results)\n",
    "#     results_json = json.loads(results)\n",
    "#     for curr_col, curr_val in results_json.items():\n",
    "#         qa_dict_final[curr_col].append(curr_val)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891133f7-9bbf-4ce5-a665-d2cebc0f0204",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "65daab50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### OBS - GPT4o mini model can now handle over 10k tokens, no longer requiring chunking\n",
    "\n",
    "# ####SUMMARIZE LONGER TRANSCRIPTS - store in summaries_dict\n",
    "# #### save checkpoints to summaries folder\n",
    "# all_summaries = []\n",
    "# summaries_dict = {\n",
    "    \n",
    "# }\n",
    "# max_len = 1800\n",
    "# threshold_perc = 0.75\n",
    "# threshold = max_len*threshold_perc\n",
    "# chunksize = 1000\n",
    "# ind = 0 #used to iterate through transcript chunks to summarize\n",
    "\n",
    "# # summary_prefix = f\"\"\"This is a transcript from a YouTube video.in {threshold} words or less, \n",
    "# #                     summarize the following text: \"\"\"\n",
    "# # classify_prefix = \"This is a transcript from a YouTube video.\"\n",
    "# for i, text in enumerate(transcript_dict['sents']):\n",
    "# # for i,text in enumerate(data['transcript_strings']):\n",
    "#     video_id = data['videoId'][i]\n",
    "    \n",
    "# #     if text and i <=250:\n",
    "#     if text and i <= max_summaries:\n",
    "#         tokens = text.split()\n",
    "# #         print(f\"FINAL LENGTH: {len(tokens)}\")\n",
    "#         if len(tokens) >= threshold_perc:\n",
    "#             summary_chunks = []\n",
    "#             for i in range(0, len(tokens),chunksize):\n",
    "#                 ###SET indices to get tokens for each chunk\n",
    "#                 if i == 0:\n",
    "# #                     print(\"---START\")\n",
    "#                     start_ind = i\n",
    "#                     end_ind = chunksize\n",
    "#                 elif len(tokens) % i < chunksize and len(tokens)//i <= 1.0:\n",
    "# #                     print(\"---END\")\n",
    "#                     start_ind = i +1\n",
    "#                     end_ind = len(tokens)\n",
    "#                 else:\n",
    "# #                     print(\"---MID\")\n",
    "#                     start_ind = i +1\n",
    "#                     end_ind = i + chunksize\n",
    "# #                 print(f\"START AND END: {start_ind} to {end_ind}\")\n",
    "# #                 print(\"****\")\n",
    "#                 to_summarize = \" \".join(tokens[start_ind:end_ind])\n",
    "# #                 print(to_summarize)\n",
    "# #                 print(\"****\")\n",
    "\n",
    "#                 summ_query = f\"The following is a portion of a YouTube video transcript. Summarize what the speaker is saying and include as many details as you can about the speaker's current and former job positions. Also include details related to their reasons for quitting: {to_summarize}\"\n",
    "# #                 print(len(summ_query.split()))\n",
    "#                 summary_completion = openai.Completion.create(\n",
    "#                     engine=model_engine,\n",
    "#                     prompt=summ_query,\n",
    "#                     max_tokens=500, #prompt restricts length of response, but max_tokens determines cutoff tokens \n",
    "#                     n=1,\n",
    "#                     stop=None,\n",
    "#                     temperature=0.5,\n",
    "#                 )\n",
    "# #                 print(summary_completion)\n",
    "#                 summary_chunks.append(summary_completion.choices[0].text)\n",
    "#             all_summaries.append(\"\\n\".join(summary_chunks))\n",
    "#             summaries_dict[video_id] = \"\\n\".join(summary_chunks)\n",
    "# #             print(len(\"\\n\".join(summary_chunks).split()))\n",
    "#         else:\n",
    "#             print(\"CLASSIFY\")\n",
    "#             continue\n",
    "#         print(\"---\")\n",
    "#     else:\n",
    "# #         print(\"NO TRANSCRIPT\")\n",
    "#         summaries_dict[video_id] = \"NO TRANSCRIPT AVAILABLE\"\n",
    "    \n",
    "#     ###ADD CHECKPOINT IN 'summaries' file - put last video id into file name\n",
    "#     if len(str(ind)) ==1:\n",
    "#         checkpoint_ind = \"00\" + str(ind)\n",
    "#     elif len(str(ind)) == 2:\n",
    "#         checkpoint_ind = \"0\" + str(ind)\n",
    "#     else:\n",
    "#         checkpoint_ind = str(ind)\n",
    "#     checkpoint_file = f\"{summ_filepath}/checkpoint_{checkpoint_ind}_{video_id}.json\"\n",
    "#     with open(checkpoint_file, 'w', encoding='utf-8') as f:\n",
    "#         json.dump(summaries_dict, f, ensure_ascii=False, indent=4)\n",
    "    \n",
    "#     if ind == max_summaries:\n",
    "#         break\n",
    "#     ind += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e6654a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3e5535",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02eb49a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b0b988fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = summaries.values()\n",
    "# qa_query = f\"This is a transcript from a YouTube video. in three separate bullet points, answer 3 questions for the following transcript: \\n What is the current job/occupation of the speaker? \\n 2. What was the former job/occupation of the speaker? \\n 3. What is the main reason that the speaker had for quitting their job/occupation? \\n {text}\"\n",
    "    \n",
    "# qa_completion = openai.Completion.create(\n",
    "#     engine=model_engine,\n",
    "#     prompt=qa_query,\n",
    "#     max_tokens=500, #prompt restricts length of response, but max_tokens determines cutoff tokens \n",
    "#     n=1,\n",
    "#     stop=None,\n",
    "#     temperature=0.5,\n",
    "# )\n",
    "\n",
    "# qa_result = qa_completion.choices[0].text\n",
    "# qa_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "04f5d8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### OBSOLETE - no longer needed due to GPT4omini version\n",
    "\n",
    "# #answer questions\n",
    "# #get last item from chkpt_filpath - last checkpoint w most summaries \n",
    "# final_summaries = sorted(os.listdir(summ_filepath))[-1]\n",
    "# summaries_filepath=f\"{summ_filepath}/{final_summaries}\"\n",
    "# with open(summaries_filepath, 'r') as f:\n",
    "#     summaries = json.load(f)\n",
    "\n",
    "# qa_dict = {}\n",
    "# qa_ind = 0 \n",
    "# for video_id, text in summaries.items():\n",
    "#     qa_query = f\"This is a transcript from a YouTube video. in three separate bullet points, answer 3 questions for the following transcript: \\n What is the current job/occupation of the speaker? \\n 2. What was the former job/occupation of the speaker? \\n 3. What is the main reason that the speaker had for quitting their job/occupation? \\n {text}\"\n",
    "#     if len(qa_query.split()) <= 3000:\n",
    "#         qa_completion = openai.Completion.create(\n",
    "#             engine=model_engine,\n",
    "#             prompt=qa_query,\n",
    "#             max_tokens=500, #prompt restricts length of response, but max_tokens determines cutoff tokens \n",
    "#             n=1,\n",
    "#             stop=None,\n",
    "#             temperature=0.5,\n",
    "#         )\n",
    "\n",
    "#         qa_result = qa_completion.choices[0].text\n",
    "# #         print(qa_result)\n",
    "#         if len(qa_result) > 0:\n",
    "#             qa_dict[video_id] = qa_result\n",
    "#         else:\n",
    "#             qa_dict[video_id] = \"No Answer Available\"\n",
    "\n",
    "#         if qa_ind % 10 == 0 or qa_ind == 500:\n",
    "#             if len(str(qa_ind)) ==1:\n",
    "#                 checkpoint_ind = \"00\" + str(qa_ind)\n",
    "#             elif len(str(qa_ind)) == 2:\n",
    "#                 checkpoint_ind = \"0\" + str(qa_ind)\n",
    "#             else:\n",
    "#                 checkpoint_ind = str(qa_ind)\n",
    "#             checkpoint_file = f\"{qa_filepath}/checkpoint_{checkpoint_ind}_{video_id}.json\"\n",
    "\n",
    "#             with open(checkpoint_file, 'w', encoding='utf-8') as f:\n",
    "#                 json.dump(qa_dict, f, ensure_ascii=False, indent=4)\n",
    "#         print(\"---\")\n",
    "#     else:\n",
    "#         print(\"****\")\n",
    "#         print(f'Transcript is too long: {video_id} {len(qa_query.split())}')\n",
    "#         print(qa_query)\n",
    "#         continue\n",
    "#     qa_ind += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "84cda158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # SAVE OUT JSON AND CSV\n",
    "# out_filepath_json = f\"{data_folder}/02_QA_results.json\"\n",
    "# with open(out_filepath_json, 'w', encoding='utf-8') as f:\n",
    "#     json.dump(qa_dict, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "02ac610a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df66d29c-0a94-416b-a3ae-f66e85ffe5f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
